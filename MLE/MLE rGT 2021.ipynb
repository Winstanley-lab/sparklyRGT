{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io as sio\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'BH04_raw_acquisition.xlsx'\n",
    "dataset = pd.read_excel(file_name)\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(dataset[dataset['Chosen']==0].index, inplace = True) ## removed all rows where 'Chosen' == 0 is true \n",
    "# dataset\n",
    "# 'Chosen == 0' on a premature response or an omission\n",
    "\n",
    "configA = np.array([1, 4, 0, 2, 3])\n",
    "configB = np.array([4, 1, 0, 3, 2])\n",
    "\n",
    "dataset['MSN'].unique() ## the 2 versions of the task\n",
    "\n",
    "dataset['option'] = dataset['MSN'].str.contains(\"B\").values*configB[dataset['Chosen'].astype('int').ravel()-1].astype('int') + \\\n",
    "    dataset['MSN'].str.contains(\"A\").values*configA[dataset['Chosen'].astype('int').ravel()-1].astype('int') \n",
    "## makes a new column called 'option' which stores P1-P4 choice depending on 'Chosen' and version of the task \n",
    "\n",
    "dataset[dataset['Subject'] == 2][['Pun_Dur','Pellets','Chosen','option']]\n",
    "## dataset where subject == 1 (1149 rows), and only shows 4 columns + 'Chosen' != 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "def mylikelihood(params, ch, rew, pun, ntrials):\n",
    "\n",
    "    # params is a 3 vector of beta, etaP, etaN\n",
    "    # ch is a vector of choices (one per trial) (P1-P4 choice)\n",
    "    # rew is a vector of rewards (Pellets)\n",
    "    # pun is a vector of punishments (Pun_Dur)\n",
    "    \n",
    "    V = np.zeros(4) ## V = latent Q values which start at 0 \n",
    "    ## V stores Q value of P1, Q(P2), Q(P3), Q(P4)\n",
    "    beta = params[0] ## takes the first number in the params vector \n",
    "    etaP = params[1]\n",
    "    etaN = params[2]\n",
    "\n",
    "    loglik = 0 ## start at 0\n",
    "    \n",
    "    for t in range(ntrials): \n",
    "    \n",
    "        # now we want to calculate the log likelihood of the choice on the current trial\n",
    "        ## ln(e) = 1\n",
    "        ## log likelihood is ln(p(P#))*** \n",
    "        ## p(Px) \"probability of the data given the parameters we pass to the function (beta, etaN, etaP)\"\n",
    "        # we assume the prob of each choice follows a softmax rule\n",
    "        # in log this looks like this\n",
    "        \n",
    "        loglik += beta*V[ch[t]-1] - logsumexp(beta*V) \n",
    "        ## ln likelihood is equal to ln(softmax rule)  \n",
    "        ## recall: the log likelihood is just the likelihood (parameters given data) with log***\n",
    "        ## += just adds to current value \n",
    "        \n",
    "        if rew[t]>0:\n",
    "            V[ch[t]-1] += etaP*(rew[t] - V[ch[t]-1])\n",
    "            # this is the same as writing\n",
    "            # V[chosen option](new) = V[chosen option](old)* + etaP*(reward - V[chosen option](old))\n",
    "            ## Note that V[chosen option](old)* isn't present in the equation due to the += \n",
    "        else: ## when rew[t] == 0\n",
    "            V[ch[t]-1] += etaN*(-pun[t] - V[ch[t]-1])\n",
    "            ## V[chosen option] = V[chosen option] + etaN*(punishment - V[chosen option])\n",
    "            ## decreases the Q value (V)\n",
    "            ## punishment is experienced time-out duration in seconds (assumes m is 1 in this example)\n",
    "\n",
    "    return -loglik #easier to minimize than maximize \n",
    "\n",
    "##function: note how loglik is accumulating in magnitude with each trial, and how only one V updates per trial \n",
    "##what does the loglik value represent?*** why do we want to minimize/maximize it?*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = range(ntrials)\n",
    "# t\n",
    "choices[t]-1 ## holds an array of (P1-P4 choice - 1) for each trial (ex. P1 - 1 = 0 in the array)\n",
    "V = np.zeros(4) \n",
    "# V\n",
    "V[choices[t]-1] ## irrelevant storage --> function:\n",
    "## V[0] stores Q(P1), therefore, we must subtract 1 from choices[t] in order to update the correct value in V\n",
    "\n",
    "beta = 10\n",
    "\n",
    "V_test = np.array([1,1,1,1])\n",
    "V_test\n",
    "\n",
    "logsumexp(beta*V_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(10)\n",
    "np.log(np.sum(np.exp(a)))\n",
    "a\n",
    "\n",
    "np.exp(a) ## e^x\n",
    "np.sum(a)\n",
    "np.sum(np.exp(a))\n",
    "# np.log(a) #ln, not log \n",
    "np.log(np.sum(np.exp(a)))\n",
    "logsumexp(a) ## same value as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE code again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out the information we need for MLE\n",
    "\n",
    "#this is for subject 2 only\n",
    "\n",
    "# .values squashes the pandas Series back to a numpy array\n",
    "## pandas Series is a one column dataframe \n",
    "## numpy array is different - can iterate through \n",
    "choices = dataset.loc[dataset['Subject']==2,'option'].values \n",
    "## for subject 2, outputs an array of P1-P4 choice (stored in 'option' column) for each trial \n",
    "## recall: 'Chosen' == 0 was removed \n",
    "\n",
    "rewards = dataset.loc[dataset['Subject']==2,'Pellets'].values \n",
    "## outputs number of pellets rewarded per trial ('Chosen != 0') --> 0 means they lost \n",
    "\n",
    "punishments = dataset.loc[dataset['Subject']==2,'Pun_Dur'].values\n",
    "## outputs punishment durations --> 0 means they won, and ex) 40 means they wait for 40sec\n",
    "\n",
    "ntrials = len(punishments)\n",
    "## punishments was arbitrary --> ntrials stores the number of trials for this subject (can we assume each subject only played one version - MSN? \n",
    "# --> yes must check)\n",
    "\n",
    "# now I can test the likelihood with some fixed parameter values...\n",
    "test_parameters = [10,0.5,0.5] ## random values \n",
    "mylikelihood(test_parameters, choices, rewards, punishments, ntrials)\n",
    "\n",
    "# now we know our function works!!!\n",
    "\n",
    "#this prints out the negative log likelihood, which is the number we want to minimize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choices = dataset.loc[dataset['Subject']==2,'option'].values\n",
    "# choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can ask the optimizer to find the parameter values that minimize the negative log likelihood calculated by our function!\n",
    "\n",
    "## test_parameters are an educated initialization\n",
    "## args = additional arguments \n",
    "## bounds are respective to beta, etaP, etaN\n",
    "# len(punishments)\n",
    "\n",
    "results = minimize(mylikelihood,test_parameters,args=(choices,rewards,punishments,ntrials),bounds=((0, None), (0, 1), (0, 1)))\n",
    "results\n",
    "## minimize --> a function that finds the parameter values that minimize the negative log-likelihood calculated by our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usually, you will iterate over 3-5 minimize calls, with different random starting points, and choose the one with the \n",
    "# highest (ie, lowest negative) likelihood (lowest -loglik)\n",
    "\n",
    "## highest loglik (least negative, closest to 0) is at the combination of parameters that best fit the data (the ML parameter estimates) \n",
    "## lowest -loglik is the same thing!^ ***\n",
    "\n",
    "init = 3 # three different initializations\n",
    "\n",
    "estimates = np.zeros([init,4]) ## stores array of zeros, 3 rows of 4 zeros each \n",
    "\n",
    "for n in range(init): ## range 0-3\n",
    "    print(n)\n",
    "    \n",
    "    res = minimize(mylikelihood,[10+np.random.rand()*10,0.5+np.random.rand()*0.2,0.5+np.random.rand()*0.2],\n",
    "                   args=(choices,rewards,punishments,ntrials),bounds=((0, None), (0, 1), (0, 1))) \n",
    "    ## res is the minimize function, with the addition of random numbers to beta, etaP and etaN \n",
    "\n",
    "    print(res.fun, res.x[0], res.x[1], res.x[2])\n",
    "    estimates[n,:] = [res.fun, res.x[0], res.x[1], res.x[2]]\n",
    "\n",
    "\n",
    "#for the print out - 1st value is the minimized negative log likelihood\n",
    "#second value is the beta parameter\n",
    "#third value is the etaP parameter\n",
    "#fourth value is the etaN parameter\n",
    "\n",
    "## res.fun calls the 'fun' field in res \n",
    "## 0 1 2 represent the 1st, 2nd and 3rd initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates_test = np.zeros((3,4)) \n",
    "estimates_test\n",
    "range(init) \n",
    "np.random.rand()\n",
    "\n",
    "np.linspace(0, 5, 11)\n",
    "np.linspace(0.005,0.1,21)\n",
    "\n",
    "np.zeros([len(beta), len(etaP)]) ## 11 rows of 21 zeros each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is useful to plot the entire likelihood surface for your data given the model\n",
    "\n",
    "beta = np.linspace(0,5,11) ##between 0 and 5, gives 11 evenly spaced out values from 0 to 5\n",
    "etaP = np.linspace(0.005,0.1,21) \n",
    "etaN = 0.00536\n",
    "\n",
    "nll = np.zeros([len(beta),len(etaP)])\n",
    "\n",
    "for b in range(len(beta)):\n",
    "    for e in range(len(etaP)):\n",
    "        \n",
    "        nll[b,e] = mylikelihood([beta[b],etaP[e],etaN],choices,rewards,punishments,ntrials)\n",
    "        \n",
    "        ## for beta = 0, etaP = 0.005 --> -loglik = 2449.58\n",
    "        ## for beta = 0, etaP = 0.00975 --> -loglik = 2449.58 \n",
    "        ## function: for each b, test the 21 etaP values, output the -loglik (from mylikelihood)\n",
    "        ## output: 11*21 -loglik values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nll stores the likelihoods for each of the parameter values stored in beta and etaP \n",
    "nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this plots the likelihood surface --> best parameter fits are at the darkest colour (minimum negative LL)\n",
    "\n",
    "#from the MLE function a few cells above, we know the best fits are at beta = 2.97, and etaP = .07\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=[15,8])\n",
    "\n",
    "im = ax.imshow(nll)\n",
    "fig.colorbar(im,ax=ax)\n",
    "ax.set(xticks=[0,20],xticklabels=[0,.1],yticks=[0,10],yticklabels=[0,5])\n",
    "\n",
    "## likelihood surface \n",
    "## slice of likelihood surface at etaN = 0.00536\n",
    "## y axis is beta, x-axis is etaP \n",
    "## axis towards me is -loglik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects\n",
    "# estimates\n",
    "# range(len(subjects)) ## issue: there is no subject 0 \n",
    "\n",
    "# now we can do this over all subjects\n",
    "\n",
    "init = 3\n",
    "subjects = dataset['Subject'].unique()\n",
    "estimates = np.zeros([len(subjects),init,4])\n",
    "for sub in range(len(subjects)):\n",
    "    print(f'Subject {subjects[sub]}...')\n",
    "    \n",
    "    choices = dataset.loc[dataset['Subject']==subjects[sub],'option'].values\n",
    "\n",
    "    rewards = dataset.loc[dataset['Subject']==subjects[sub],'Pellets'].values\n",
    "\n",
    "    punishments = dataset.loc[dataset['Subject']==subjects[sub],'Pun_Dur'].values\n",
    "    \n",
    "    ntrials = len(punishments)\n",
    "    \n",
    "    for n in range(init):\n",
    "        print(n)\n",
    "        res_test = minimize(mylikelihood,[10+np.random.rand()*10,0.5+np.random.rand()*0.2,0.5+np.random.rand()*0.2],\n",
    "                       args=(choices,rewards,punishments,ntrials),bounds=((0, None), (0, 1), (0, 1)))\n",
    "    \n",
    "        print(res.fun, res.x[0], res.x[1], res.x[2])\n",
    "        estimates[sub,n,:] = [res.fun, res.x[0], res.x[1], res.x[2]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
