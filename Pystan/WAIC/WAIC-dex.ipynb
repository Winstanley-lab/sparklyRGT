{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am being executed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import arviz as az\n",
    "# import scipy as sp\n",
    "# import scipy.io as sio\n",
    "# import scipy.stats as stats\n",
    "# from scipy.optimize import minimize\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "# import pickle\n",
    "# import importlib\n",
    "import math\n",
    "import xarray as xr\n",
    "os.chdir(\"C:\\\\Users\\\\dexte\\\\sparklyRGT\\\\sparklyRGT_tutorial\") \n",
    "import sparklyRGT as rgt\n",
    "os.chdir(\"C:\\\\Users\\\\dexte\\\\sparklyRGT\\\\Pystan\") \n",
    "import model_data as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('nc_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_basic_fit = az.from_netcdf('standard_basic_fit.nc')\n",
    "uncued_basic_fit = az.from_netcdf('uncued_basic_fit.nc')\n",
    "loss_basicstar_fit = az.from_netcdf('loss_basic-star_fit.nc')\n",
    "loss_pscale_fit = az.from_netcdf('loss_pscale_fit.nc')\n",
    "loss_pscalestar_fit = az.from_netcdf('loss_pscale-star_fit.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']}\n",
    "data_df = pd.DataFrame.from_dict(data)\n",
    "data_df.to_excel(\"data_df.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_basicstar_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_pscale_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_pscalestar_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbf_df = standard_basic_fit.posterior.beta.to_dataframe()\n",
    "sbf_df_etaP = standard_basic_fit.posterior.etaPositive.to_dataframe()\n",
    "sbf_df_etaP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_values = sbf_df[sbf_df.index.get_level_values('beta_dim_0').isin([0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etaP_values = sbf_df_etaP[sbf_df_etaP.index.get_level_values('etaPositive_dim_0').isin([0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([beta_values, etaP_values], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubf_m_df = uncued_basic_fit.posterior.m.to_dataframe()\n",
    "ubf_m_df[ubf_m_df.index.get_level_values('m_dim_0').isin([0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_basic_4_chains(fit, s):\n",
    "    \"\"\"takes in fit, s (subject number), and outputs params_s using ALL CHAINS\n",
    "    function is called basic due to the parameters chosen\"\"\"\n",
    "    \n",
    "    fit_beta_df = fit.posterior.beta.to_dataframe()\n",
    "    fit_etaP_df = fit.posterior.etaPositive.to_dataframe()\n",
    "    fit_etaN_df = fit.posterior.etaNegative.to_dataframe()\n",
    "    fit_m_df = fit.posterior.m.to_dataframe()\n",
    "    \n",
    "    params_s = np.concatenate([fit_beta_df[fit_beta_df.index.get_level_values('beta_dim_0').isin([s])],\n",
    "                              fit_etaP_df[fit_etaP_df.index.get_level_values('etaPositive_dim_0').isin([s])],\n",
    "                              fit_etaN_df[fit_etaN_df.index.get_level_values('etaNegative_dim_0').isin([s])],\n",
    "                              fit_m_df[fit_m_df.index.get_level_values('m_dim_0').isin([s])]],\n",
    "                              axis = 1)\n",
    "    return params_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_all_4 = get_params_basic_4_chains(standard_basic_fit, 0)\n",
    "params_all_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_basic_fit.posterior.beta #beta's DataArray \n",
    "# standard_basic_fit.posterior.beta.beta_dim_0 #beta's subjects\n",
    "standard_basic_fit.posterior.beta[0] #first chain, storing an array of beta values\n",
    "chain_1_beta = standard_basic_fit.posterior.beta[0] #first chain, each column represents a subject; each column stores the posterior values of beta\n",
    "chain_1_beta_df = pd.DataFrame(data = chain_1_beta) #chain_1 as a df\n",
    "chain_1_beta[:,0] #first column of the first chain, representing the beta values of the posterior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_1_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_basic_fit.posterior.etaPositive #etaPositive's DataArray\n",
    "standard_basic_fit.posterior.etaPositive[0] #first chain\n",
    "chain_1_etaP = standard_basic_fit.posterior.etaPositive[0]\n",
    "\n",
    "standard_basic_fit.posterior.etaNegative #etaNegative's DataArray\n",
    "standard_basic_fit.posterior.etaNegative[0] #first chain\n",
    "chain_1_etaN = standard_basic_fit.posterior.etaNegative[0]\n",
    "\n",
    "standard_basic_fit.posterior.m #m's DataArray \n",
    "standard_basic_fit.posterior.m[0] #first chain\n",
    "chain_1_m = standard_basic_fit.posterior.m[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_1_etaP\n",
    "chain_1_etaN\n",
    "chain_1_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_1_etaP[:,[0]]\n",
    "chain_1_etaN[:,[0]]\n",
    "chain_1_m[:,[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: create params for one subject (params_s)\n",
    "\n",
    "params_s will be 4 columns (representing beta, etaP, etaN and m) and 1000 rows (representing the 1000 draws)\n",
    "\n",
    "## Goal 2: write for loop such that the code can run multiple times, using a different subject each time ***\n",
    "- This will involve taking the first column from each parameter's dataarray (for subject 0), appending them (side-by-side), then running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = np.array([[1, 1, 1], [2, 2, 2]])\n",
    "array2 = np.array([[3, 3, 3], [4, 4, 4]])\n",
    "\n",
    "appended = np.concatenate([array1, array2], axis = 1)\n",
    "appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = np.array([[1,2,3]])\n",
    "array2 = np.array([[4,5,6]])\n",
    "\n",
    "appended = np.concatenate([array1, array2], axis = 0)\n",
    "# appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = np.concatenate([chain_1_beta[:,[0,1]], chain_1_etaP[:,[0,1]], chain_1_etaN[:,[0,1]]], axis = 1)\n",
    "# parameters #a workaround, but contains 2 beta values, 2 etaP, and 2etaN (as the 6 columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_1 = np.concatenate([chain_1_beta[:,[0]], chain_1_etaP[:,[0]], chain_1_etaN[:,[0]], chain_1_m[:,[0]]], axis = 1) #extra square parentheses add a dimension\n",
    "params_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_basic(fit, s):\n",
    "    \"\"\"takes in fit, s (subject number), and outputs params_s using the first chain\n",
    "    function is called basic due to the parameters chosen\"\"\"\n",
    "    params_s = np.concatenate([fit.posterior.beta[0][:,[s-1]], \n",
    "                               fit.posterior.etaPositive[0][:,[s-1]], \n",
    "                               fit.posterior.etaNegative[0][:,[s-1]]], \n",
    "                              axis = 1)\n",
    "    return params_s\n",
    "\n",
    "#fit.posterior.m[0][:,[s-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_1 = get_params_basic(standard_basic_fit, 1)\n",
    "params_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(fit, subs): \n",
    "    \"\"\"gets params for subs, this function is not functional yet\"\"\"\n",
    "    for s in range(subs):\n",
    "        params = np.concatenate([fit.posterior.beta[:,[s]], fit.posterior.etaPositive[:,[s]], fit.posterior.etaNegative[:,[s]]], axis = 1)\n",
    "        #... rest of body \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_params(standard_basic_fit, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original simulation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(\n",
    "ntr = 200 # num trials #might be 1000\n",
    "\n",
    "def mysimulation(params,ntr):\n",
    "    \n",
    "    # params is a 3 vector of beta, etaP, etaN\n",
    "    \n",
    "    V = np.zeros(4) # each option has a value\n",
    "    beta = params[0]\n",
    "    etaP = params[1]\n",
    "    etaN = params[2]\n",
    "    decay = params[3] # decay between zero and 1\n",
    "    \n",
    "#     V[3] = 4\n",
    "    \n",
    "    p_win = [0.9,0.8,0.5,0.4]\n",
    "    win_amount = [1,2,3,4]\n",
    "    pun_dur = [5,10,30,40]\n",
    "    \n",
    "    Q = np.zeros([4,ntr])\n",
    "    choice = []\n",
    "    win = []\n",
    "    probs = []\n",
    "    \n",
    "    \n",
    "    for t in range(ntr):\n",
    "#         print(t)\n",
    "\n",
    "        Q[:,t] = V\n",
    "        \n",
    "        # now we want to calculate the log likelihood of the choice on the current trial\n",
    "        # we assume the prob of each choice follows a softmax rule\n",
    "        # in log this looks like this\n",
    "        \n",
    "        p_action = np.exp(beta*V)/np.sum(np.exp(beta*V))\n",
    "        \n",
    "        # pick an action according to these probabilities\n",
    "        chosen = np.random.choice([0,1,2,3], size=None, replace=True, p=p_action)\n",
    "    \n",
    "        # now we want to learn from feedback on this trial\n",
    "        # win or lose?\n",
    "        outcome = np.random.choice([1,0], size=None, replace=True, p=[p_win[chosen], 1-p_win[chosen]])\n",
    "        \n",
    "        if outcome:\n",
    "            V[chosen] += etaP*(win_amount[chosen] - V[chosen])\n",
    "            # this is the same as writing\n",
    "            # V[chosen option] = V[chosen option] + eta*(reward - V[chosen option])\n",
    "        else:\n",
    "            V[chosen] += etaN*(-pun_dur[chosen] - V[chosen])\n",
    "        \n",
    "        # values decay with time if unchosen\n",
    "        ind = np.setdiff1d([0,1,2,3],chosen)\n",
    "#         print(ind)\n",
    "        V[ind] = decay*V[ind]\n",
    "        \n",
    "        choice.append(chosen)\n",
    "        win.append(outcome)\n",
    "        probs.append(p_action)\n",
    "        \n",
    "    return Q,choice,win,probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Simulation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params is a 3 vector of beta, etaP, etaN\n",
    "params = [2.52937956, 0.02981436, 0.11405636]\n",
    "ntr = 1000\n",
    "\n",
    "V = np.zeros(4) # [0,0,0,0]\n",
    "beta = params[0]\n",
    "etaP = params[1]\n",
    "etaN = params[2]\n",
    "# decay = params[3] # decay between zero and 1\n",
    "    \n",
    "p_win = [0.9,0.8,0.5,0.4]\n",
    "win_amount = [1,2,3,4]\n",
    "pun_dur = [5,10,30,40]\n",
    "\n",
    "Q = np.zeros([4,ntr]) #1 array of 4 rows and 1000 columns\n",
    "choice = []\n",
    "win = []\n",
    "probs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q\n",
    "arr = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\n",
    "arr[2,:] = V\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(ntr): #from 0 to 999\n",
    "\n",
    "    Q[:,t] = V #each column of Q (which represents a t) is now equal to V (4 zeros). This essentially makes each column equal to 4 rows of zero\n",
    "\n",
    "    p_action = np.exp(beta*V)/np.sum(np.exp(beta*V)) #p_action holds the probabilities of each action (P1-P4), starting at 0.25 for each\n",
    "\n",
    "    chosen = np.random.choice([0,1,2,3], size=None, replace=True, p=p_action) #a simulated sample of choices (0-3 rep: P1-P4) using the probabilities from p_action\n",
    "#     print(chosen)\n",
    "    outcome = np.random.choice([1,0], size=None, replace=True, p=[p_win[chosen], 1-p_win[chosen]]) #a simulated sample of outcomes based on the choice made (in chosen)\n",
    "\n",
    "    if outcome: #if outcome == 1:\n",
    "        V[chosen] += etaP*(win_amount[chosen] - V[chosen])\n",
    "        # this is the same as writing\n",
    "        # V[chosen option] = V[chosen option] + eta*(reward - V[chosen option])\n",
    "    else: #if outcome == 0:\n",
    "        V[chosen] += etaN*(-pun_dur[chosen] - V[chosen])\n",
    "\n",
    "    choice.append(chosen)\n",
    "    win.append(outcome)\n",
    "    probs.append(p_action)\n",
    "\n",
    "# return Q,choice,win,probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(probs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 3: we now need to acquire actual chosen and outcome data, and ntr... for a certain subject "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\dexte\\\\sparklyRGT\\\\Pystan\") \n",
    "fnames = ['BH09_raw-free_S1-5_corrected.xlsx','CH02_corrected.xlsx','NA01_raw_free-choice_S8-18.xlsx',\"CH01_corrected.xlsx\"]\n",
    "df = rgt.load_multiple_data(fnames, reset_sessions = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename MSNs so that the rats on the outcome task don't have \"loss\" in the MSN\n",
    "for i in range(len(df)):\n",
    "    if df.at[i, 'MSN'] == 'LossrGT_A-losscue_v1':\n",
    "        df.at[i,'MSN'] = 'outcomeRGT_A'\n",
    "    if df.at[i, 'MSN'] == 'LossrGT_B-losscue_v1':\n",
    "        df.at[i,'MSN'] = 'outcomeRGT_B'\n",
    "        \n",
    "#rename MSNs so that the rats on the random task don't have \"loss\" in the MSN\n",
    "for i in range(len(df)):\n",
    "    if df.at[i,'MSN'] == 'AnarchyrGT_B-losscue_v6':\n",
    "        df.at[i,'MSN'] = 'RandomRGT_B'\n",
    "    if df.at[i,'MSN'] == 'AnarchyrGT_A-losscue_v6':\n",
    "        df.at[i,'MSN'] = 'RandomRGT_A'\n",
    "        \n",
    "        \n",
    "task_list = df.groupby(['MSN'])['Subject'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncued_subs = np.concatenate(task_list[[task for task in df.MSN.unique() if 'Classic' in task]])\n",
    "standard_subs = np.concatenate((task_list['rGT_A-cue'], task_list['rGT_B-cue']))\n",
    "#concatenating together MisRGT tasks, and RevRGT tasks, as they both refer to reverse-cue RGT\n",
    "reverse_subs = np.concatenate((np.concatenate(task_list[[task for task in df.MSN.unique() if 'Mis' in task]]),\n",
    "                              np.concatenate(task_list[[task for task in df.MSN.unique() if 'Rev' in task]])))\n",
    "outcome_subs = np.concatenate(task_list[[task for task in df.MSN.unique() if 'outcome' in task]])\n",
    "random_subs = np.concatenate(task_list[[task for task in df.MSN.unique() if 'Random' in task]])\n",
    "loss_subs = np.concatenate(task_list[[task for task in df.MSN.unique() if 'oss' in task]])\n",
    "\n",
    "subs = [uncued_subs,standard_subs,reverse_subs,outcome_subs,random_subs,loss_subs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncued_subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numsessions = 5 #***first 5 sessions?\n",
    "uncued = md.get_model_data(df, numsessions, subs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncued_df = pd.DataFrame.from_dict(uncued)\n",
    "uncued_df \n",
    "# C (chosen hole of 5), R (reward in pellets, 0 if loss), P (pun_dur, >0 if loss), O (P1-P4 option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = uncued_df.startSubject.unique()\n",
    "np.delete(subjects, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncued_df = uncued_df.drop(uncued_df[(uncued_df.C == 0) & (uncued_df.startSubject == 0)].index)\n",
    "uncued_df.reset_index(inplace = True)\n",
    "uncued_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in uncued_df.index:\n",
    "    if uncued_df.at[row,'R'] == 0: #loss\n",
    "        uncued_df.at[row,'outcome'] = 0\n",
    "    if uncued_df.at[row,'P'] == 0: #win\n",
    "        uncued_df.at[row,'outcome'] = 1\n",
    "    \n",
    "uncued_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncued_df.index[uncued_df['startSubject'] == 2].tolist()\n",
    "value = uncued_df.index[uncued_df['startSubject'] == 2]\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncued_1 = uncued_df[0:849] #df for subject 1 \n",
    "uncued_1[['O', 'outcome']]\n",
    "chosen_data = uncued_1['O']\n",
    "outcome_data = uncued_1['outcome']\n",
    "outcome_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 4: after acquiring chosen_data (option, O) and outcome_data (0 or 1 depending on R and P), we now run the model on the data to acquire p_action for each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr = 509\n",
    "for t in range(ntr): #from 0 to 508\n",
    "\n",
    "    Q[:,t] = V #each column of Q (which represents a t) is now equal to V (4 zeros). This essentially makes each column equal to 4 rows of zero\n",
    "\n",
    "    p_action = np.exp(beta*V)/np.sum(np.exp(beta*V)) #p_action holds the probabilities of each action (P1-P4), starting at 0.25 for each\n",
    "\n",
    "    if outcome_data[t] == 1: #if outcome == 1:\n",
    "        V[chosen_data[t]-1] += etaP*(win_amount[chosen_data[t]-1] - V[chosen_data[t]-1]) #subtract 1 because chosen_data stores P1-P4 as 1-4 instead 0-3\n",
    "    else: #if outcome_data == 0:\n",
    "        V[chosen_data[t]-1] += etaN*(-pun_dur[chosen_data[t]-1] - V[chosen_data[t]-1])\n",
    "\n",
    "    probs.append(p_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 5: we now want the log(p_action) for the option chosen. \n",
    "\n",
    "ex. p_action = [0.245, 0.28, 0.235, 0.24] ... and P4 was then chosen. We would take log(0.24) for this trial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(probs)\n",
    "# probs #list of 509 arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chosen_data)\n",
    "# chosen_data #series object of 509 choices (1-4 representing P1-P4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.log10(10)\n",
    "probs[0][0]\n",
    "math.log10(probs[0][0]) #first array, first value\n",
    "log_lik = pd.DataFrame()\n",
    "log_lik\n",
    "math.log10(probs[0][chosen_data[1]-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'col1': [0]}\n",
    "log_lik_test = pd.DataFrame(d)\n",
    "log_lik_test\n",
    "log_lik_test[f\"log_lik[{0}]\"] = math.log10(probs[0][chosen_data[1]-1])\n",
    "log_lik_test\n",
    "log_lik = pd.DataFrame(d)\n",
    "\n",
    "# log_lik_test.assign(log_lik_test=[92,81,66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(probs)): \n",
    "    log_lik[f\"log_lik[{t}]\"] = math.log10(probs[t][chosen_data[t]-1])\n",
    "    \n",
    "log_lik['sum'] = log_lik.sum(axis = 1)\n",
    "log_lik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 6: we now want each column (ex. log_lik[5]) to contain 1000 values (each value is a log likelihood) \n",
    "\n",
    "Procedure: \n",
    "- Figure out how to add a value to the next row of the same column (ex. add the next value to log_lik[0] as another row)\n",
    "    - either add a list as a new row\n",
    "    - or, add a single value to the bottom of a column\n",
    "- Form list comprehension of trials and params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'col1': np.zeros(3)}\n",
    "log_lik_test = pd.DataFrame(d)\n",
    "log_lik_test.loc[log_lik_test.index[2], 'col1'] = 4\n",
    "# df.loc[df.index[someRowNumber], 'New Column Title'] = \"some value\"\n",
    "log_lik_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr = 849\n",
    "\n",
    "paramsXtrial = [(p, t) for p in range(4000) for t in range(ntr)]\n",
    "# paramsXtrial\n",
    "\n",
    "# for (p, t) in paramsXtrial: #one set of parameters, for all the trials, then the next set of parameters (params[1]) for all the trials, etc. \n",
    "#     print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final goal: summarize the code (skip goal 4 and 5) \n",
    "- subject 1 of uncued_subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params is a 3 vector of beta, etaP, etaN\n",
    "import math\n",
    "params = params_all_4 #1000 sets of parameters\n",
    "\n",
    "V = np.zeros(4) # [0,0,0,0]\n",
    "# decay = params[3] # decay between zero and 1\n",
    "    \n",
    "p_win = [0.9,0.8,0.5,0.4]\n",
    "win_amount = [1,2,3,4]\n",
    "pun_dur = [5,10,30,40]\n",
    "ntr = 849\n",
    "\n",
    "Q = np.zeros([4,ntr]) #1 array of 4 rows and ntr columns\n",
    "probs = []\n",
    "\n",
    "d = {'col1': np.zeros(1000)}\n",
    "log_lik = pd.DataFrame(d)\n",
    "\n",
    "for (p, t) in paramsXtrial: #from 0 to 508\n",
    "\n",
    "    Q[:,t] = V #each column of Q (which represents a t) is now equal to V (4 zeros). This essentially makes each column equal to 4 rows of zero\n",
    "    \n",
    "    if t == 0: #if using a new set of parameters (if first trial, reset)...\n",
    "        V = np.zeros(4) #reset the V values\n",
    "\n",
    "    p_action = np.exp(params[p][0]*V)/np.sum(np.exp(params[p][0]*V)) #p_action holds the probabilities of each action (P1-P4), starting at 0.25 for each\n",
    "\n",
    "    if outcome_data[t] == 1: #if outcome == 1:\n",
    "        V[chosen_data[t]-1] += params[p][1]*(win_amount[chosen_data[t]-1] - V[chosen_data[t]-1]) #subtract 1 because chosen_data stores P1-P4 as 1-4 instead 0-3\n",
    "    else: #if outcome_data == 0:\n",
    "        V[chosen_data[t]-1] += params[p][2]*(-pun_dur[chosen_data[t]-1] - V[chosen_data[t]-1])\n",
    "         \n",
    "    log_lik.loc[log_lik.index[p], f\"log_lik[{t}]\"] = math.log10(p_action[chosen_data[t]-1]) #may want to change to natural log (ln) \n",
    "    \n",
    "    probs.append(p_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up: write function(s)\n",
    "- remove col1 \n",
    "- transform a sum column, or take the sum of the entire dataframe (log_lik) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs should be real probabilities\n",
    "\n",
    "# Q,choice,win,probs = mysimulation([2,0.1,0.02,0.9],ntr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=[12,5])\n",
    "\n",
    "ax[0].plot(np.array(probs))\n",
    "ax[0].legend(['1','2','3','4'])\n",
    "\n",
    "ax[1].plot(np.arange(ntr),Q.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check out what softmax does to choice probabilties for diff values\n",
    "\n",
    "def softmax(x,beta):\n",
    "    \n",
    "    return np.exp(beta*x)/np.sum(np.exp(beta*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([10,40],softmax(np.array([10,40]),0),'*')\n",
    "plt.plot([10,40],softmax(np.array([10,40]),0.5),'*')\n",
    "plt.plot([10,40],softmax(np.array([10,40]),0.01),'*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_liks = log_lik.drop('col1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_liks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_liks['row sum'] = log_liks.sum(axis = 1) #watch out! if you run this cell multiple times, it will continue adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_log_lik = log_liks['row sum'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_log_lik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function (loglik_basic_sub) \n",
    "\n",
    "- parameters \n",
    "    - outcome_data\n",
    "    - chosen_data \n",
    "    - params \n",
    "    - sub\n",
    "\n",
    "- function template \n",
    "    - set variables (V, Q) \n",
    "    - set model (p_win, win_amount, pun_dur) \n",
    "    - 3 helper functions: \n",
    "        - get_outcome_data \n",
    "        - get_chosen_data\n",
    "        - get_ntr_sub\n",
    "    - run each ntr (should be a range from starting trial to end trial) X sub combination through the model \n",
    "    - output: sum of the log-likelihoods, for all subjects!\n",
    "    \n",
    "Workflow: \n",
    "- create df with subject, range(ntr), chosen_data, outcome_data as the columns \n",
    "- create df for each subject (by calling data in a row) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def md_subs(df, numsessions, subs): \n",
    "    \"\"\"gets md_df and unique subs (without subject 0)\"\"\"\n",
    "    md_dict = md.get_model_data(df, numsessions, subs) #model data as a dict\n",
    "    md_df = pd.DataFrame.from_dict(md_dict) #model data as a df\n",
    "    \n",
    "    subs_unique = md_df.startSubject.unique()\n",
    "    subs_unique = np.delete(subs_unique, 1) #remove subject 0 (not a real subject)\n",
    "    ###could just take the max subject and list(range())\n",
    "    \n",
    "    return md_df, subs_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntr_sub(md_df, subs_unique): \n",
    "    \"\"\"ntr_sub takes in md_df, and gets the range of trials for a given subject within subs\"\"\"\n",
    "    \n",
    "    ntr_sub_ranges = []\n",
    "    ntr_df = pd.DataFrame({'subject#': subs_unique})\n",
    "    \n",
    "    #get range for sub: \n",
    "    for s in subs_unique: \n",
    "        first_t = md_df.index[md_df['startSubject'] == s].tolist() #first trial\n",
    "        last_t = md_df.index[md_df['startSubject'] == (s+1)].tolist() #last trial\n",
    "        \n",
    "        if last_t != []: #if s+1 does not exist (when s = final subject#), last_t = []\n",
    "            ntr_sub = list(range(first_t[0], last_t[0]))\n",
    "            ntr_sub_ranges.append(ntr_sub)\n",
    "        elif last_t == []: ### could just do one if statement\n",
    "            last_t = [len(md_df)] #last_t = length of md_df\n",
    "            ntr_sub = list(range(first_t[0], last_t[0]))\n",
    "            ntr_sub_ranges.append(ntr_sub)\n",
    "        \n",
    "    ntr_df['range_ntr'] = ntr_sub_ranges\n",
    "    return ntr_df #outputs df with subject, and range_ntr as the next column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ranges = ntr_sub(df, 5, [202, 218, 222, 310, 314, 401, 403, 405, 407, 415, 204, 220, 224, 309, 313, 402, 404, 406, 408, 416, 325, 329, 326, 330])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chosen_outcome_data(md_df, ntr_df, subs_unique): \n",
    "    \"\"\"takes in md_df, subs_unique and gets chosen and outcome data\"\"\"\n",
    "    \n",
    "    for row in md_df.index: #add outcome column (where loss == 0, and win == 1)\n",
    "        if md_df.at[row,'R'] == 0: #loss\n",
    "            md_df.at[row,'outcome'] = 0\n",
    "        if md_df.at[row,'P'] == 0: #win\n",
    "            md_df.at[row,'outcome'] = 1\n",
    "    \n",
    "    chosen = []\n",
    "    outcome = []\n",
    "    ntr_df = ntr_sub(md_df, subs_unique)\n",
    "    \n",
    "    for row in ntr_df.index:\n",
    "        range_ts = ntr_df.at[row, 'range_ntr'] #range of trials\n",
    "        filtered_md_df = md_df[range_ts[0]:(range_ts[-1]+1)]\n",
    "        chosen.append(filtered_md_df['O'].values)\n",
    "        outcome.append(filtered_md_df['outcome'].values)\n",
    "                               \n",
    "    return chosen, outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_params_*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_basic(fit, subs_unique):\n",
    "    \"\"\"takes in fit, subs_unique (list from [1, final sub#]), and outputs params_s using ALL CHAINS\n",
    "    function is called basic due to the parameters chosen\"\"\"\n",
    "    \n",
    "    fit_beta_df = fit.posterior.beta.to_dataframe()\n",
    "    fit_etaP_df = fit.posterior.etaPositive.to_dataframe()\n",
    "    fit_etaN_df = fit.posterior.etaNegative.to_dataframe()\n",
    "    fit_m_df = fit.posterior.m.to_dataframe()\n",
    "    \n",
    "    params_s = []\n",
    "    \n",
    "    for s in subs_unique:\n",
    "        params = np.concatenate([fit_beta_df[fit_beta_df.index.get_level_values('beta_dim_0').isin([s-1])],\n",
    "                                  fit_etaP_df[fit_etaP_df.index.get_level_values('etaPositive_dim_0').isin([s-1])],\n",
    "                                  fit_etaN_df[fit_etaN_df.index.get_level_values('etaNegative_dim_0').isin([s-1])],\n",
    "                                  fit_m_df[fit_m_df.index.get_level_values('m_dim_0').isin([s-1])]],\n",
    "                                  axis = 1)\n",
    "        params_s.append(params)\n",
    "    return params_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_basicstar(fit, subs_unique):\n",
    "    \"\"\"takes in fit, subs_unique (list from [1, final sub#]), and outputs params_s using ALL CHAINS\n",
    "    function is called basic due to the parameters chosen\"\"\"\n",
    "    \n",
    "    fit_beta_df = fit.posterior.beta.to_dataframe()\n",
    "    fit_eta_df = fit.posterior.eta.to_dataframe()\n",
    "    fit_m_df = fit.posterior.m.to_dataframe()\n",
    "    \n",
    "    params_s = []\n",
    "    \n",
    "    for s in subs_unique:\n",
    "        params = np.concatenate([fit_beta_df[fit_beta_df.index.get_level_values('beta_dim_0').isin([s-1])],\n",
    "                                  fit_eta_df[fit_eta_df.index.get_level_values('eta_dim_0').isin([s-1])],\n",
    "                                  fit_m_df[fit_m_df.index.get_level_values('m_dim_0').isin([s-1])]],\n",
    "                                  axis = 1)\n",
    "        params_s.append(params)\n",
    "    return params_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_basicstar_params = get_params_basicstar(loss_basicstar_fit, [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_pscale(fit, subs_unique):\n",
    "    \"\"\"takes in fit, subs_unique (list from [1, final sub#]), and outputs params_s using ALL CHAINS\n",
    "    function is called basic due to the parameters chosen\"\"\"\n",
    "    \n",
    "    fit_beta_df = fit.posterior.beta.to_dataframe()\n",
    "    fit_etaP_df = fit.posterior.etaPositive.to_dataframe()\n",
    "    fit_etaN_df = fit.posterior.etaNegative.to_dataframe()\n",
    "    fit_m_df = fit.posterior.m.to_dataframe()\n",
    "    fit_b_df = fit.posterior.b.to_dataframe()\n",
    "    \n",
    "    params_s = []\n",
    "    \n",
    "    for s in subs_unique:\n",
    "        params = np.concatenate([fit_beta_df[fit_beta_df.index.get_level_values('beta_dim_0').isin([s-1])],\n",
    "                                  fit_etaP_df[fit_etaP_df.index.get_level_values('etaPositive_dim_0').isin([s-1])],\n",
    "                                  fit_etaN_df[fit_etaN_df.index.get_level_values('etaNegative_dim_0').isin([s-1])],\n",
    "                                  fit_m_df[fit_m_df.index.get_level_values('m_dim_0').isin([s-1])],\n",
    "                                  fit_b_df[fit_b_df.index.get_level_values('b_dim_0').isin([s-1])]],\n",
    "                                  axis = 1)\n",
    "        params_s.append(params)\n",
    "    return params_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_pscale_params = get_params_pscale(loss_pscale_fit, [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24])\n",
    "# loss_pscale_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen1, outcome1 = chosen_outcome_data(df, 5, [202, 218, 222, 310, 314, 401, 403, 405, 407, 415, 204, 220, 224, 309, 313, 402, 404, 406, 408, 416, 325, 329, 326, 330])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_pscalestar(fit, subs_unique):\n",
    "    \"\"\"takes in fit, subs_unique (list from [1, final sub#]), and outputs params_s using ALL CHAINS\n",
    "    function is called basic due to the parameters chosen\"\"\"\n",
    "    \n",
    "    fit_beta_df = fit.posterior.beta.to_dataframe()\n",
    "    fit_eta_df = fit.posterior.eta.to_dataframe()\n",
    "    fit_m_df = fit.posterior.m.to_dataframe()\n",
    "    fit_b_df = fit.posterior.b.to_dataframe()\n",
    "    \n",
    "    params_s = []\n",
    "    \n",
    "    for s in subs_unique:\n",
    "        params = np.concatenate([fit_beta_df[fit_beta_df.index.get_level_values('beta_dim_0').isin([s-1])],\n",
    "                                  fit_eta_df[fit_eta_df.index.get_level_values('eta_dim_0').isin([s-1])],\n",
    "                                  fit_m_df[fit_m_df.index.get_level_values('m_dim_0').isin([s-1])],\n",
    "                                  fit_b_df[fit_b_df.index.get_level_values('b_dim_0').isin([s-1])]],\n",
    "                                  axis = 1)\n",
    "        params_s.append(params)\n",
    "    return params_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_pscalestar_params = get_params_pscalestar(loss_pscalestar_fit, [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_pindep(fit, subs_unique):\n",
    "    \"\"\"takes in fit, subs_unique (list from [1, final sub#]), and outputs params_s using ALL CHAINS\n",
    "    function is called basic due to the parameters chosen\"\"\"\n",
    "    \n",
    "    fit_beta_df = fit.posterior.beta.to_dataframe()\n",
    "    fit_etaP_df = fit.posterior.etaPositive.to_dataframe()\n",
    "    fit_etaN_df = fit.posterior.etaNegative.to_dataframe()\n",
    "    fit_p1_df = fit.posterior.p1.to_dataframe()\n",
    "    fit_p2_df = fit.posterior.p2.to_dataframe()\n",
    "    fit_p3_df = fit.posterior.p3.to_dataframe()\n",
    "    fit_p4_df = fit.posterior.p4.to_dataframe()\n",
    "    \n",
    "    params_s = []\n",
    "    \n",
    "    for s in subs_unique:\n",
    "        params = np.concatenate([fit_beta_df[fit_beta_df.index.get_level_values('beta_dim_0').isin([s-1])],\n",
    "                                  fit_etaP_df[fit_etaP_df.index.get_level_values('etaPositive_dim_0').isin([s-1])],\n",
    "                                  fit_etaN_df[fit_etaN_df.index.get_level_values('etaNegative_dim_0').isin([s-1])],\n",
    "                                  fit_m_df[fit_p1_df.index.get_level_values('p1_dim_0').isin([s-1])],\n",
    "                                  fit_m_df[fit_p2_df.index.get_level_values('p2_dim_0').isin([s-1])],\n",
    "                                  fit_m_df[fit_p3_df.index.get_level_values('p3_dim_0').isin([s-1])],\n",
    "                                  fit_m_df[fit_p4_df.index.get_level_values('p4_dim_0').isin([s-1])]],\n",
    "                                  axis = 1)\n",
    "        params_s.append(params)\n",
    "    return params_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_pindepstar(fit, subs_unique): ###make one function\n",
    "    \"\"\"takes in fit, subs_unique (list from [1, final sub#]), and outputs params_s using ALL CHAINS\n",
    "    function is called basic due to the parameters chosen\"\"\"\n",
    "    \n",
    "    fit_beta_df = fit.posterior.beta.to_dataframe()\n",
    "    fit_eta_df = fit.posterior.eta.to_dataframe()\n",
    "    fit_p1_df = fit.posterior.p1.to_dataframe()\n",
    "    fit_p2_df = fit.posterior.p2.to_dataframe()\n",
    "    fit_p3_df = fit.posterior.p3.to_dataframe()\n",
    "    fit_p4_df = fit.posterior.p4.to_dataframe()\n",
    "    \n",
    "    params_s = []\n",
    "    \n",
    "    for s in subs_unique:\n",
    "        params = np.concatenate([fit_beta_df[fit_beta_df.index.get_level_values('beta_dim_0').isin([s-1])],\n",
    "                                  fit_eta_df[fit_eta_df.index.get_level_values('eta_dim_0').isin([s-1])],\n",
    "                                  fit_m_df[fit_p1_df.index.get_level_values('p1_dim_0').isin([s-1])],\n",
    "                                  fit_m_df[fit_p2_df.index.get_level_values('p2_dim_0').isin([s-1])],\n",
    "                                  fit_m_df[fit_p3_df.index.get_level_values('p3_dim_0').isin([s-1])],\n",
    "                                  fit_m_df[fit_p4_df.index.get_level_values('p4_dim_0').isin([s-1])]],\n",
    "                                  axis = 1)\n",
    "        params_s.append(params)\n",
    "    return params_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_df_basic(df, numsessions, subs, fit): ### model parameter\n",
    "    \"\"\"takes the output from md_subs, ntr_sub and chosen_outcome_data functions and outputs the complete df\"\"\"\n",
    "    md_df, subs_unique = md_subs(df, numsessions, subs)\n",
    "    ntr_df = ntr_sub(md_df, subs_unique)\n",
    "    chosen_data, outcome_data = chosen_outcome_data(md_df, ntr_df, subs_unique)\n",
    "    params_s = get_params_basic(fit, subs_unique)\n",
    "    \n",
    "    ntr_df['chosen_data'] = chosen_data\n",
    "    ntr_df['outcome_data'] = outcome_data\n",
    "    ntr_df['params_s'] = params_s\n",
    "    \n",
    "    return ntr_df, md_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_df_basicstar(df, numsessions, subs, fit):\n",
    "    \"\"\"takes the output from md_subs, ntr_sub and chosen_outcome_data functions and outputs the complete df\"\"\"\n",
    "    md_df, subs_unique = md_subs(df, numsessions, subs)\n",
    "    ntr_df = ntr_sub(md_df, subs_unique)\n",
    "    chosen_data, outcome_data = chosen_outcome_data(md_df, ntr_df, subs_unique)\n",
    "    params_s = get_params_basicstar(fit, subs_unique)\n",
    "    \n",
    "    ntr_df['chosen_data'] = chosen_data\n",
    "    ntr_df['outcome_data'] = outcome_data\n",
    "    ntr_df['params_s'] = params_s\n",
    "    \n",
    "    return ntr_df, md_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_df_pscale(df, numsessions, subs, fit):\n",
    "    \"\"\"takes the output from md_subs, ntr_sub and chosen_outcome_data functions and outputs the complete df\"\"\"\n",
    "    md_df, subs_unique = md_subs(df, numsessions, subs)\n",
    "    ntr_df = ntr_sub(md_df, subs_unique)\n",
    "    chosen_data, outcome_data = chosen_outcome_data(md_df, ntr_df, subs_unique)\n",
    "    params_s = get_params_pscale(fit, subs_unique)\n",
    "    \n",
    "    ntr_df['chosen_data'] = chosen_data\n",
    "    ntr_df['outcome_data'] = outcome_data\n",
    "    ntr_df['params_s'] = params_s\n",
    "    \n",
    "    return ntr_df, md_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_df_pscalestar(df, numsessions, subs, fit):\n",
    "    \"\"\"takes the output from md_subs, ntr_sub and chosen_outcome_data functions and outputs the complete df\"\"\"\n",
    "    md_df, subs_unique = md_subs(df, numsessions, subs)\n",
    "    ntr_df = ntr_sub(md_df, subs_unique)\n",
    "    chosen_data, outcome_data = chosen_outcome_data(md_df, ntr_df, subs_unique)\n",
    "    params_s = get_params_pscalestar(fit, subs_unique)\n",
    "    \n",
    "    ntr_df['chosen_data'] = chosen_data\n",
    "    ntr_df['outcome_data'] = outcome_data\n",
    "    ntr_df['params_s'] = params_s\n",
    "    \n",
    "    return ntr_df, md_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_df_pindep(df, numsessions, subs, fit):\n",
    "    \"\"\"takes the output from md_subs, ntr_sub and chosen_outcome_data functions and outputs the complete df\"\"\"\n",
    "    md_df, subs_unique = md_subs(df, numsessions, subs)\n",
    "    ntr_df = ntr_sub(md_df, subs_unique)\n",
    "    chosen_data, outcome_data = chosen_outcome_data(md_df, ntr_df, subs_unique)\n",
    "    params_s = get_params_pindep(fit, subs_unique)\n",
    "    \n",
    "    ntr_df['chosen_data'] = chosen_data\n",
    "    ntr_df['outcome_data'] = outcome_data\n",
    "    ntr_df['params_s'] = params_s\n",
    "    \n",
    "    return ntr_df, md_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_df_pindepstar(df, numsessions, subs, fit):\n",
    "    \"\"\"takes the output from md_subs, ntr_sub and chosen_outcome_data functions and outputs the complete df\"\"\"\n",
    "    md_df, subs_unique = md_subs(df, numsessions, subs)\n",
    "    ntr_df = ntr_sub(md_df, subs_unique)\n",
    "    chosen_data, outcome_data = chosen_outcome_data(md_df, ntr_df, subs_unique)\n",
    "    params_s = get_params_pindepstar(fit, subs_unique)\n",
    "    \n",
    "    ntr_df['chosen_data'] = chosen_data\n",
    "    ntr_df['outcome_data'] = outcome_data\n",
    "    ntr_df['params_s'] = params_s\n",
    "    \n",
    "    return ntr_df, md_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df, md_df = sub_df_basic(df, 5, [201, 205], standard_basic_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df ###very high beta value in subject 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# md_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does len(chosen_data) match range_ntr\n",
    "There's 0's in chosen_data - resolved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncued_basic_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncued_4_chains = uncued_basic_fit.to_dataframe()\n",
    "uncued_4_chains\n",
    "uncued_1_chain = uncued_4_chains.iloc[0:1000]\n",
    "uncued_1_chain\n",
    "uncued_1_chain_no_index = uncued_1_chain.reset_index(drop = True)\n",
    "uncued_1_chain_no_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_liks = log_lik.drop('col1', axis=1)\n",
    "log_liks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncued_1_chain_no_index.iloc[:,[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([uncued_1_chain_no_index.iloc[:,[0,1]], log_liks], axis = 1)\n",
    "df_concat #df_concat is the complete dataframe for subject 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = df_concat.set_index([\"chain\", \"draw\"])\n",
    "x_concat = xr.Dataset.from_dataframe(df_concat)\n",
    "x_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_concat = az.InferenceData(sample_stats=x_concat)\n",
    "dataset_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# az.waic(dataset_concat, var_name = 'log_lik')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add df as an xarray to inferenceData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_liks = log_lik.drop('col1', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_liks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xarray example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xr.Dataset(\n",
    "    {\n",
    "        \"temperature_c\": (\n",
    "            (\"lat\", \"lon\"),\n",
    "            20 * np.random.rand(4).reshape(2, 2),\n",
    "        ),\n",
    "        \"precipitation\": ((\"lat\", \"lon\"), np.random.rand(4).reshape(2, 2)),\n",
    "    },\n",
    "    coords={\"lat\": [10, 20], \"lon\": [150, 160]},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.assign(temperature_f=x[\"temperature_c\"] * 9 / 5 + 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_f=x[\"temperature_c\"] * 9 / 5 + 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to_xarray example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([('falcon', 'bird', 389.0, 2),\n",
    "                   ('parrot', 'bird', 24.0, 2),\n",
    "                   ('lion', 'mammal', 80.5, 4),\n",
    "                   ('monkey', 'mammal', np.nan, 4)],\n",
    "                  columns=['name', 'class', 'max_speed',\n",
    "                           'num_legs'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['max_speed'].to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_liks_dataset = log_liks.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncued_basic_fit_log_lik = uncued_basic_fit.assign(log_liks_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncued_basic_fit_log_lik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.waic(uncued_basic_fit_log_lik, var_name = log_lik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centered eight example of az.waic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = az.load_arviz_data(\"centered_eight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample_stats.log_likelihood.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indices are chain, draw, trial\n",
    "and the values are log_likelihood values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_liks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiindex example --> completed with log_likelihood added to standard_basic_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr = sub_df.at[1, 'range_ntr']\n",
    "ntr[-1]\n",
    "# ntr[-1] - ntr[0] # 445\n",
    "# list(range(ntr[-1]+1 - ntr[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsXtrial = [(p, t) for p in range(4000) for t in range(ntr[-1] - ntr[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_log_lik_values(sub_df):\n",
    "    \"\"\"takes in sub_df, and gets all the log_likelihood values in a list\"\"\"\n",
    "    \n",
    "    log_lik_values = [] #stores log_lik_values for all subjects\n",
    "    \n",
    "    #model\n",
    "    p_win = [0.9,0.8,0.5,0.4]\n",
    "    win_amount = [1,2,3,4]\n",
    "    pun_dur = [5,10,30,40]\n",
    "    \n",
    "    for s in sub_df.index: #note: s is 0-indexed\n",
    "        \n",
    "        #subject data\n",
    "        ntr = sub_df.at[s,'range_ntr']\n",
    "        chosen_data = sub_df.at[s,'chosen_data']\n",
    "        outcome_data = sub_df.at[s,'outcome_data']\n",
    "        params = sub_df.at[s, 'params_s'] #4000 sets of parameters \n",
    "        \n",
    "        #updating model values\n",
    "        V = np.zeros(4) # [0,0,0,0]\n",
    "#         Q = np.zeros([4,ntr]) #1 array of 4 rows and ntr columns\n",
    "        \n",
    "        paramsXtrial = [(p, t) for p in range(4000) for t in range(ntr[-1]+1 - ntr[0])] #4000 sets of parameters, for ntr trials\n",
    "        for (p, t) in paramsXtrial: \n",
    "\n",
    "            if t == 0: #if using a new set of parameters (if first trial, reset)...\n",
    "                V = np.zeros(4) #reset the V values\n",
    "\n",
    "            p_action = np.exp(params[p][0]*V)/np.sum(np.exp(params[p][0]*V)) #p_action holds the probabilities of each action (P1-P4), starting at 0.25 for each\n",
    "\n",
    "            if outcome_data[t] == 1: \n",
    "                V[chosen_data[t]-1] += params[p][1]*(win_amount[chosen_data[t]-1] - V[chosen_data[t]-1]) #subtract 1 because chosen_data stores P1-P4 as 1-4 instead 0-3\n",
    "            else: \n",
    "                V[chosen_data[t]-1] += params[p][2]*(-params[p][3]*pun_dur[chosen_data[t]-1] - V[chosen_data[t]-1])\n",
    "            log_lik_values.append(math.log(p_action[chosen_data[t]-1])) \n",
    "    \n",
    "    return log_lik_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basicstar_log_lik_values(sub_df):\n",
    "    \"\"\"takes in sub_df, and gets all the log_likelihood values in a list\"\"\"\n",
    "    \n",
    "    log_lik_values = [] #stores log_lik_values for all subjects\n",
    "    \n",
    "    #model\n",
    "    p_win = [0.9,0.8,0.5,0.4]\n",
    "    win_amount = [1,2,3,4]\n",
    "    pun_dur = [5,10,30,40]\n",
    "    \n",
    "    for s in sub_df.index: #note: s is 0-indexed\n",
    "        \n",
    "        #subject data\n",
    "        ntr = sub_df.at[s,'range_ntr']\n",
    "        chosen_data = sub_df.at[s,'chosen_data']\n",
    "        outcome_data = sub_df.at[s,'outcome_data']\n",
    "        params = sub_df.at[s, 'params_s'] #4000 sets of parameters \n",
    "        \n",
    "        #updating model values\n",
    "        V = np.zeros(4) # [0,0,0,0]\n",
    "#         Q = np.zeros([4,ntr]) #1 array of 4 rows and ntr columns\n",
    "        \n",
    "        paramsXtrial = [(p, t) for p in range(4000) for t in range(ntr[-1]+1 - ntr[0])] #4000 sets of parameters, for ntr trials\n",
    "        for (p, t) in paramsXtrial: \n",
    "\n",
    "            if t == 0: #if using a new set of parameters (if first trial, reset)...\n",
    "                V = np.zeros(4) #reset the V values\n",
    "\n",
    "            p_action = np.exp(params[p][0]*V)/np.sum(np.exp(params[p][0]*V)) #p_action holds the probabilities of each action (P1-P4), starting at 0.25 for each\n",
    "\n",
    "            if outcome_data[t] == 1: \n",
    "                V[chosen_data[t]-1] += params[p][1]*(win_amount[chosen_data[t]-1] - V[chosen_data[t]-1]) #subtract 1 because chosen_data stores P1-P4 as 1-4 instead 0-3\n",
    "            else: \n",
    "                V[chosen_data[t]-1] += params[p][1]*(-params[p][2]*pun_dur[chosen_data[t]-1] - V[chosen_data[t]-1])\n",
    "            log_lik_values.append(math.log(p_action[chosen_data[t]-1])) \n",
    "    \n",
    "    return log_lik_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pscale_log_lik_values(sub_df):\n",
    "    \"\"\"takes in sub_df, and gets all the log_likelihood values in a list\"\"\"\n",
    "    \n",
    "    log_lik_values = [] #stores log_lik_values for all subjects\n",
    "    \n",
    "    #model\n",
    "    p_win = [0.9,0.8,0.5,0.4]\n",
    "    win_amount = [1,2,3,4]\n",
    "    pun_dur = [5,10,30,40]\n",
    "    \n",
    "    for s in sub_df.index: #note: s is 0-indexed\n",
    "        \n",
    "        #subject data\n",
    "        ntr = sub_df.at[s,'range_ntr']\n",
    "        chosen_data = sub_df.at[s,'chosen_data']\n",
    "        outcome_data = sub_df.at[s,'outcome_data']\n",
    "        params = sub_df.at[s, 'params_s'] #4000 sets of parameters \n",
    "        \n",
    "        #updating model values\n",
    "        V = np.zeros(4) # [0,0,0,0]\n",
    "#         Q = np.zeros([4,ntr]) #1 array of 4 rows and ntr columns\n",
    "        \n",
    "        paramsXtrial = [(p, t) for p in range(4000) for t in range(ntr[-1]+1 - ntr[0])] #4000 sets of parameters, for ntr trials\n",
    "        for (p, t) in paramsXtrial: \n",
    "\n",
    "            if t == 0: #if using a new set of parameters (if first trial, reset)...\n",
    "                V = np.zeros(4) #reset the V values\n",
    "\n",
    "            p_action = np.exp(params[p][0]*V)/np.sum(np.exp(params[p][0]*V)) #p_action holds the probabilities of each action (P1-P4), starting at 0.25 for each\n",
    "\n",
    "            if outcome_data[t] == 1: \n",
    "                V[chosen_data[t]-1] += params[p][1]*(win_amount[chosen_data[t]-1] - V[chosen_data[t]-1]) #subtract 1 because chosen_data stores P1-P4 as 1-4 instead 0-3\n",
    "            else: \n",
    "                V[chosen_data[t]-1] += params[p][2]*(params[p][4] - params[p][3]*pun_dur[chosen_data[t]-1] - V[chosen_data[t]-1])\n",
    "            log_lik_values.append(math.log(p_action[chosen_data[t]-1])) \n",
    "    \n",
    "    return log_lik_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pscalestar_log_lik_values(sub_df):\n",
    "    \"\"\"takes in sub_df, and gets all the log_likelihood values in a list\"\"\"\n",
    "    \n",
    "    log_lik_values = [] #stores log_lik_values for all subjects\n",
    "    \n",
    "    #model\n",
    "    p_win = [0.9,0.8,0.5,0.4]\n",
    "    win_amount = [1,2,3,4]\n",
    "    pun_dur = [5,10,30,40]\n",
    "    \n",
    "    for s in sub_df.index: #note: s is 0-indexed\n",
    "        \n",
    "        #subject data\n",
    "        ntr = sub_df.at[s,'range_ntr']\n",
    "        chosen_data = sub_df.at[s,'chosen_data']\n",
    "        outcome_data = sub_df.at[s,'outcome_data']\n",
    "        params = sub_df.at[s, 'params_s'] #4000 sets of parameters \n",
    "        \n",
    "        #updating model values\n",
    "        V = np.zeros(4) # [0,0,0,0]\n",
    "#         Q = np.zeros([4,ntr]) #1 array of 4 rows and ntr columns\n",
    "        \n",
    "        paramsXtrial = [(p, t) for p in range(4000) for t in range(ntr[-1]+1 - ntr[0])] #4000 sets of parameters, for ntr trials\n",
    "        for (p, t) in paramsXtrial: \n",
    "\n",
    "            if t == 0: #if using a new set of parameters (if first trial, reset)...\n",
    "                V = np.zeros(4) #reset the V values\n",
    "\n",
    "            p_action = np.exp(params[p][0]*V)/np.sum(np.exp(params[p][0]*V)) #p_action holds the probabilities of each action (P1-P4), starting at 0.25 for each\n",
    "\n",
    "            if outcome_data[t] == 1: \n",
    "                V[chosen_data[t]-1] += params[p][1]*(win_amount[chosen_data[t]-1] - V[chosen_data[t]-1]) #subtract 1 because chosen_data stores P1-P4 as 1-4 instead 0-3\n",
    "            else: \n",
    "                V[chosen_data[t]-1] += params[p][1]*(params[p][3] - params[p][2]*pun_dur[chosen_data[t]-1] - V[chosen_data[t]-1])\n",
    "            log_lik_values.append(math.log(p_action[chosen_data[t]-1])) \n",
    "    \n",
    "    return log_lik_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pindep_log_lik_values(sub_df):\n",
    "    \"\"\"takes in sub_df, and gets all the log_likelihood values in a list\"\"\"\n",
    "    \n",
    "    log_lik_values = [] #stores log_lik_values for all subjects\n",
    "    \n",
    "    #model\n",
    "    p_win = [0.9,0.8,0.5,0.4]\n",
    "    win_amount = [1,2,3,4]\n",
    "    pun_dur = [5,10,30,40]\n",
    "    \n",
    "    for s in sub_df.index: #note: s is 0-indexed\n",
    "        \n",
    "        #subject data\n",
    "        ntr = sub_df.at[s,'range_ntr']\n",
    "        chosen_data = sub_df.at[s,'chosen_data']\n",
    "        outcome_data = sub_df.at[s,'outcome_data']\n",
    "        params = sub_df.at[s, 'params_s'] #4000 sets of parameters \n",
    "        \n",
    "        #updating model values\n",
    "        V = np.zeros(4) # [0,0,0,0]\n",
    "#         Q = np.zeros([4,ntr]) #1 array of 4 rows and ntr columns\n",
    "        \n",
    "        paramsXtrial = [(p, t) for p in range(4000) for t in range(ntr[-1]+1 - ntr[0])] #4000 sets of parameters, for ntr trials\n",
    "        for (p, t) in paramsXtrial: \n",
    "\n",
    "            if t == 0: #if using a new set of parameters (if first trial, reset)...\n",
    "                V = np.zeros(4) #reset the V values\n",
    "\n",
    "            p_action = np.exp(params[p][0]*V)/np.sum(np.exp(params[p][0]*V)) #p_action holds the probabilities of each action (P1-P4), starting at 0.25 for each\n",
    "\n",
    "            if outcome_data[t] == 1: \n",
    "                V[chosen_data[t]-1] += params[p][1]*(win_amount[chosen_data[t]-1] - V[chosen_data[t]-1]) #subtract 1 because chosen_data stores P1-P4 as 1-4 instead 0-3\n",
    "            else: \n",
    "                V[chosen_data[t]-1] += params[p][2]*(-params[p][chosen_data[t]+2]*pun_dur[chosen_data[t]-1] - V[chosen_data[t]-1]) #add 2 (ex. P1 is stored as 1\n",
    "                # but p1 is stored as params[p][3])\n",
    "            log_lik_values.append(math.log(p_action[chosen_data[t]-1])) \n",
    "    \n",
    "    return log_lik_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pindepstar_log_lik_values(sub_df):\n",
    "    \"\"\"takes in sub_df, and gets all the log_likelihood values in a list\"\"\"\n",
    "    \n",
    "    log_lik_values = [] #stores log_lik_values for all subjects\n",
    "    \n",
    "    #model\n",
    "    p_win = [0.9,0.8,0.5,0.4]\n",
    "    win_amount = [1,2,3,4]\n",
    "    pun_dur = [5,10,30,40]\n",
    "    \n",
    "    for s in sub_df.index: #note: s is 0-indexed\n",
    "        \n",
    "        #subject data\n",
    "        ntr = sub_df.at[s,'range_ntr']\n",
    "        chosen_data = sub_df.at[s,'chosen_data']\n",
    "        outcome_data = sub_df.at[s,'outcome_data']\n",
    "        params = sub_df.at[s, 'params_s'] #4000 sets of parameters \n",
    "        \n",
    "        #updating model values\n",
    "        V = np.zeros(4) # [0,0,0,0]\n",
    "#         Q = np.zeros([4,ntr]) #1 array of 4 rows and ntr columns\n",
    "        \n",
    "        paramsXtrial = [(p, t) for p in range(4000) for t in range(ntr[-1]+1 - ntr[0])] #4000 sets of parameters, for ntr trials\n",
    "        for (p, t) in paramsXtrial: \n",
    "\n",
    "            if t == 0: #if using a new set of parameters (if first trial, reset)...\n",
    "                V = np.zeros(4) #reset the V values\n",
    "\n",
    "            p_action = np.exp(params[p][0]*V)/np.sum(np.exp(params[p][0]*V)) #p_action holds the probabilities of each action (P1-P4), starting at 0.25 for each\n",
    "\n",
    "            if outcome_data[t] == 1: \n",
    "                V[chosen_data[t]-1] += params[p][1]*(win_amount[chosen_data[t]-1] - V[chosen_data[t]-1]) #subtract 1 because chosen_data stores P1-P4 as 1-4 instead 0-3\n",
    "            else: \n",
    "                V[chosen_data[t]-1] += params[p][1]*(-params[p][chosen_data[t]+1]*pun_dur[chosen_data[t]-1] - V[chosen_data[t]-1]) #add 1 (ex. P1 is stored as 1, \n",
    "                # but p1 is stored as params[p][2])\n",
    "            log_lik_values.append(math.log(p_action[chosen_data[t]-1])) \n",
    "    \n",
    "    return log_lik_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lik_values = basic_log_lik_values(sub_df) #24 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_lik_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params is a 3 vector of beta, etaP, etaN\n",
    "params = params_all_4 #1000 sets of parameters\n",
    "\n",
    "V = np.zeros(4) # [0,0,0,0]\n",
    "# decay = params[3] # decay between zero and 1\n",
    "    \n",
    "p_win = [0.9,0.8,0.5,0.4]\n",
    "win_amount = [1,2,3,4]\n",
    "pun_dur = [5,10,30,40]\n",
    "ntr = 849\n",
    "\n",
    "Q = np.zeros([4,ntr]) #1 array of 4 rows and ntr columns\n",
    "probs = []\n",
    "\n",
    "s = []\n",
    "for (p, t) in paramsXtrial: #from 0 to 508\n",
    "    \n",
    "    if t == 0: #if using a new set of parameters (if first trial, reset)...\n",
    "        V = np.zeros(4) #reset the V values\n",
    "\n",
    "    p_action = np.exp(params[p][0]*V)/np.sum(np.exp(params[p][0]*V)) #p_action holds the probabilities of each action (P1-P4), starting at 0.25 for each\n",
    "\n",
    "    if outcome_data[t] == 1: #if outcome == 1:\n",
    "        V[chosen_data[t]-1] += params[p][1]*(win_amount[chosen_data[t]-1] - V[chosen_data[t]-1]) #subtract 1 because chosen_data stores P1-P4 as 1-4 instead 0-3\n",
    "    else: #if outcome_data == 0:\n",
    "        V[chosen_data[t]-1] += params[p][2]*(-pun_dur[chosen_data[t]-1] - V[chosen_data[t]-1])\n",
    "         \n",
    "    s.append(math.log10(p_action[chosen_data[t]-1])) #may want to change to natural log (ln) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lik_values = s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(log_lik_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add log_lik_values to MultiIndex (chain, draw, trial) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterables = [[0,1,2,3], list(range(0,1000)), list(range(0, 25091))] #4 chains, 1000 draws, ntr trials\n",
    "index = pd.MultiIndex.from_product(iterables, names=[\"chain\", \"draw\", \"trial\"])\n",
    "log_lik_s = pd.Series(log_lik_values, index=index)\n",
    "df_log_likelihood = log_lik_s.to_frame(name = \"log_likelihood\")\n",
    "x_log_likelihood = df_log_likelihood.to_xarray()\n",
    "\n",
    "id_log_likelihood = standard_basic_fit.assign(x_log_likelihood)\n",
    "id_log_likelihood #2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_data_log_lik(md_df, log_lik_values, fit):\n",
    "    \"\"\"takes in log_lik_values from get_log_lik_values, and creates an inference data object\"\"\"\n",
    "    \n",
    "    total_ntr = md_df.at[0, 'ntr'] #total trials in md_df, which represents the total number of trials for a cue variant \n",
    "    print(total_ntr)\n",
    "    print(4000)\n",
    "    print(len(log_lik_values))\n",
    "    \n",
    "    #build index\n",
    "    iterables = [[0,1,2,3], list(range(0,1000)), list(range(0, total_ntr))] #4 chains, 1000 draws, total_ntr trials\n",
    "    index = pd.MultiIndex.from_product(iterables, names=[\"chain\", \"draw\", \"trial\"])\n",
    "    \n",
    "    #build InferenceData (id) object \n",
    "    s_log_likelihood = pd.Series(log_lik_values, index=index)\n",
    "    df_log_likelihood = s_log_likelihood.to_frame(name = \"log_likelihood\")\n",
    "    x_log_likelihood = df_log_likelihood.to_xarray()\n",
    "    id_log_likelihood = fit.assign(x_log_likelihood)\n",
    "    \n",
    "    return id_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_inference_data_log_lik(md_df, log_lik_values, standard_basic_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### waic_*****_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def waic_basic_fit(df, numsessions, subs, fit):\n",
    "    \"\"\"summary function: takes in all parameters and outputs WAIC table using az.waic() // parameters:\n",
    "    df - product of rgt.load_multiple_data,\n",
    "    numsessions = 5,\n",
    "    subs = cue variant (list), \n",
    "    fit = .nc object storing the model and fit\"\"\"\n",
    "    \n",
    "    sub_df, md_df = sub_df_basic(df, numsessions, subs, fit) #calculating subs twice \n",
    "    log_lik_values = basic_log_lik_values(sub_df)\n",
    "    id_log_likelihood = get_inference_data_log_lik(md_df, log_lik_values, fit)\n",
    "    \n",
    "    waic = az.waic(id_log_likelihood)\n",
    "    return waic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waic_basic_fit(df, 5, [201, 205], standard_basic_fit) #201 and 205 are the first two standard_subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def waic_basicstar_fit(df, numsessions, subs, fit):\n",
    "    \"\"\"summary function: takes in all parameters and outputs WAIC table using az.waic() // parameters:\n",
    "    df - product of rgt.load_multiple_data,\n",
    "    numsessions = 5,\n",
    "    subs = cue variant (list), \n",
    "    fit = .nc object storing the model and fit\"\"\"\n",
    "    \n",
    "    sub_df, md_df = sub_df_basicstar(df, numsessions, subs, fit)\n",
    "    log_lik_values = basicstar_log_lik_values(sub_df)\n",
    "    id_log_likelihood = get_inference_data_log_lik(md_df, log_lik_values, fit)\n",
    "    \n",
    "    waic = az.waic(id_log_likelihood)\n",
    "    return waic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waic_basicstar_fit(df, 5, loss_subs, loss_basicstar_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def waic_pscale_fit(df, numsessions, subs, fit):\n",
    "    \"\"\"summary function: takes in all parameters and outputs WAIC table using az.waic() // parameters:\n",
    "    df - product of rgt.load_multiple_data,\n",
    "    numsessions = 5,\n",
    "    subs = cue variant (list), \n",
    "    fit = .nc object storing the model and fit\"\"\"\n",
    "    \n",
    "    sub_df, md_df = sub_df_pscale(df, numsessions, subs, fit)\n",
    "    log_lik_values = pscale_log_lik_values(sub_df)\n",
    "    id_log_likelihood = get_inference_data_log_lik(md_df, log_lik_values, fit)\n",
    "    \n",
    "    waic = az.waic(id_log_likelihood)\n",
    "    return waic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waic_pscale_fit(df, 5, loss_subs, loss_pscale_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def waic_pscalestar_fit(df, numsessions, subs, fit):\n",
    "    \"\"\"summary function: takes in all parameters and outputs WAIC table using az.waic() // parameters:\n",
    "    df - product of rgt.load_multiple_data,\n",
    "    numsessions = 5,\n",
    "    subs = cue variant (list), \n",
    "    fit = .nc object storing the model and fit\"\"\"\n",
    "    \n",
    "    sub_df, md_df = sub_df_pscalestar(df, numsessions, subs, fit)\n",
    "    log_lik_values = pscalestar_log_lik_values(sub_df)\n",
    "    id_log_likelihood = get_inference_data_log_lik(md_df, log_lik_values, fit)\n",
    "    \n",
    "    waic = az.waic(id_log_likelihood)\n",
    "    return waic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12788\n",
      "4000\n",
      "51152000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dexte\\anaconda3\\lib\\site-packages\\arviz\\stats\\stats.py:692: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Computed from 4000 by 12788 log-likelihood matrix\n",
       "\n",
       "         Estimate       SE\n",
       "elpd_loo -18863.40     6.43\n",
       "p_loo    10150.85        -\n",
       "\n",
       "There has been a warning during the calculation. Please check the results."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waic_pscalestar_fit(df, 5, loss_subs, loss_pscalestar_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def waic_pindep_fit(df, numsessions, subs, fit):\n",
    "    \"\"\"summary function: takes in all p-indep parameters and outputs WAIC table using az.waic() //\n",
    "    df - product of rgt.load_multiple_data,\n",
    "    numsessions = 5,\n",
    "    subs = cue variant (list), \n",
    "    fit = .nc object storing the model and fit\"\"\"\n",
    "    \n",
    "    sub_df, md_df = sub_df_pindep(df, numsessions, subs, fit)\n",
    "    log_lik_values = pindep_log_lik_values(sub_df)\n",
    "    id_log_likelihood = get_inference_data_log_lik(md_df, log_lik_values, fit)\n",
    "    \n",
    "    waic = az.waic(id_log_likelihood)\n",
    "    return waic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def waic_pindepstar_fit(df, numsessions, subs, fit):\n",
    "    \"\"\"summary function: takes in all p-indep-star parameters and outputs WAIC table using az.waic() //\n",
    "    df - product of rgt.load_multiple_data,\n",
    "    numsessions = 5,\n",
    "    subs = cue variant (list), \n",
    "    fit = .nc object storing the model and fit\"\"\"\n",
    "    \n",
    "    sub_df, md_df = sub_df_pindepstar(df, numsessions, subs, fit)\n",
    "    log_lik_values = pindepstar_log_lik_values(sub_df)\n",
    "    id_log_likelihood = get_inference_data_log_lik(md_df, log_lik_values, fit)\n",
    "    \n",
    "    waic = az.waic(id_log_likelihood)\n",
    "    return waic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
